<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Enterprise Security Patch Management with Deep Reinforcement Learning | Qian Jia</title>
<meta name=keywords content="Security Patch Management,Reinforcement Learning"><meta name=description content="This study addresses the patching decision problem by proposing a novel deep reinforcement learning-based approach. Presented in ICIS 2024."><meta name=author content="Qian Jia,&#8201;Xinxue Qu,&#8201;Zhengrui Jiang,&#8201;Chengjun Wang"><link rel=canonical href=https://qian-jia.github.io/papers/security-patch-rl-icis/><link rel=stylesheet href=https://chinese-fonts-cdn.deno.dev/packages/LxgwNeoZhiSong/dist/LXGWNeoZhiSong/result.css><link crossorigin=anonymous href=/assets/css/stylesheet.d6c5f5ed0d983959e3280265badf3b545ec092c32db95aaa0b66c656531367e5.css integrity="sha256-1sX17Q2YOVnjKAJlut87VF7AksMtuVqqC2bGVlMTZ+U=" rel="preload stylesheet" as=style><link rel=icon href=https://qian-jia.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://qian-jia.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://qian-jia.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://qian-jia.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://qian-jia.github.io/papers/security-patch-rl-icis/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Enterprise Security Patch Management with Deep Reinforcement Learning"><meta property="og:description" content="This study addresses the patching decision problem by proposing a novel deep reinforcement learning-based approach. Presented in ICIS 2024."><meta property="og:type" content="article"><meta property="og:url" content="https://qian-jia.github.io/papers/security-patch-rl-icis/"><meta property="article:section" content="papers"><meta property="article:published_time" content="2024-12-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-15T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Enterprise Security Patch Management with Deep Reinforcement Learning"><meta name=twitter:description content="This study addresses the patching decision problem by proposing a novel deep reinforcement learning-based approach. Presented in ICIS 2024."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Papers","item":"https://qian-jia.github.io/papers/"},{"@type":"ListItem","position":2,"name":"Enterprise Security Patch Management with Deep Reinforcement Learning","item":"https://qian-jia.github.io/papers/security-patch-rl-icis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Enterprise Security Patch Management with Deep Reinforcement Learning","name":"Enterprise Security Patch Management with Deep Reinforcement Learning","description":"This study addresses the patching decision problem by proposing a novel deep reinforcement learning-based approach. Presented in ICIS 2024.","keywords":["Security Patch Management","Reinforcement Learning"],"articleBody":" Download Paper Abstract Patching in a timely manner has proven to be one of the most effective ways to protect enterprise information systems from cyberattacks. However, as patching operations are not cost-free, enterprises typically delay patch deployments. Balancing operational expenses and system security risks to determine the optimal timing of patching remains an ongoing challenge. This study addresses the patching decision problem by proposing a novel deep reinforcement learning-based approach. Specifically, we model the patching problem as a Markov decision process with a thorough consideration of various costs, dynamics, and uncertainties. To avoid the curse of dimensionality and obtain an effective patching policy, we develop a novel reinforcement learning method called Action-Decomposed Proximal Policy Optimization (ADPPO). Experimental results indicate that the proposed approach significantly outperforms benchmarks. This study contributes to both the cybersecurity management and the reinforcement learning communities.\nCitation Jia, Qian; Qu, Xinxue; Jiang, Zhengrui; and Wang, Chengjun, “Enterprise Security Patch Management with Deep Reinforcement Learning” (2024). ICIS 2024 Proceedings. 6. https://aisel.aisnet.org/icis2024/security/security/6\n","wordCount":"163","inLanguage":"en","datePublished":"2024-12-15T00:00:00Z","dateModified":"2024-12-15T00:00:00Z","author":[{"@type":"Person","name":"Qian Jia"},{"@type":"Person","name":"Xinxue Qu"},{"@type":"Person","name":"Zhengrui Jiang"},{"@type":"Person","name":"Chengjun Wang"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://qian-jia.github.io/papers/security-patch-rl-icis/"},"publisher":{"@type":"Organization","name":"Qian Jia","logo":{"@type":"ImageObject","url":"https://qian-jia.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{equation*}",right:"\\end{equation*}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1})})</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://qian-jia.github.io/ accesskey=h title="Qian Jia"><img src=https://qian-jia.github.io/favicon.ico alt aria-label=logo height=18 width=18>Qian Jia</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://qian-jia.github.io/papers/ title=Papers><span>Papers</span></a></li><li><a href=https://qian-jia.github.io/blogs/ title=Blog><span>Blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Enterprise Security Patch Management with Deep Reinforcement Learning</h1><div class=post-meta><span title='2024-12-15 00:00:00 +0000 UTC'>December 2024</span>&nbsp;&#183;&nbsp;Qian Jia,&#8201;Xinxue Qu,&#8201;Zhengrui Jiang,&#8201;Chengjun Wang&nbsp;&#183;&nbsp;<a href=https://aisel.aisnet.org/icis2024/security/security/6/ rel="noopener noreferrer" target=_blank>ICIS 2024</a></div></header><div class=post-content><hr><h5 id=download>Download</h5><ul><li><a href=https://aisel.aisnet.org/icis2024/security/security/6/ target=_blank>Paper</a></li></ul><hr><h5 id=abstract>Abstract</h5><p>Patching in a timely manner has proven to be one of the most effective ways to protect enterprise information systems from cyberattacks. However, as patching operations are not cost-free, enterprises typically delay patch deployments. Balancing operational expenses and system security risks to determine the optimal timing of patching remains an ongoing challenge. This study addresses the patching decision problem by proposing a novel deep reinforcement learning-based approach. Specifically, we model the patching problem as a Markov decision process with a thorough consideration of various costs, dynamics, and uncertainties. To avoid the curse of dimensionality and obtain an effective patching policy, we develop a novel reinforcement learning method called Action-Decomposed Proximal Policy Optimization (ADPPO). Experimental results indicate that the proposed approach significantly outperforms benchmarks. This study contributes to both the cybersecurity management and the reinforcement learning communities.</p><hr><h5 id=citation>Citation</h5><p>Jia, Qian; Qu, Xinxue; Jiang, Zhengrui; and Wang, Chengjun, &ldquo;Enterprise Security Patch Management with Deep Reinforcement Learning&rdquo; (2024). ICIS 2024 Proceedings. 6. <a href=https://aisel.aisnet.org/icis2024/security/security/6 target=_blank>https://aisel.aisnet.org/icis2024/security/security/6</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://qian-jia.github.io/tags/security-patch-management/>Security Patch Management</a></li><li><a href=https://qian-jia.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://qian-jia.github.io/>Qian Jia</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>