<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>An Introduction to Markov Chains | Qian Jia</title>
<meta name=keywords content="Markov Chain"><meta name=description content="Markov chains represent a fascinating intersection of probability and dynamic systems. They offer a framework for modeling processes where the future state depends solely on the current state—a concept known as the Markov property."><meta name=author content="Qian Jia"><link rel=canonical href=https://qian-jia.github.io/blogs/intro-markov-chain/><link rel=stylesheet href=https://chinese-fonts-cdn.deno.dev/packages/LxgwNeoZhiSong/dist/LXGWNeoZhiSong/result.css><link crossorigin=anonymous href=/assets/css/stylesheet.d6c5f5ed0d983959e3280265badf3b545ec092c32db95aaa0b66c656531367e5.css integrity="sha256-1sX17Q2YOVnjKAJlut87VF7AksMtuVqqC2bGVlMTZ+U=" rel="preload stylesheet" as=style><link rel=icon href=https://qian-jia.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://qian-jia.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://qian-jia.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://qian-jia.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://qian-jia.github.io/blogs/intro-markov-chain/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="An Introduction to Markov Chains"><meta property="og:description" content="Markov chains represent a fascinating intersection of probability and dynamic systems. They offer a framework for modeling processes where the future state depends solely on the current state—a concept known as the Markov property."><meta property="og:type" content="article"><meta property="og:url" content="https://qian-jia.github.io/blogs/intro-markov-chain/"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2025-04-03T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-03T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="An Introduction to Markov Chains"><meta name=twitter:description content="Markov chains represent a fascinating intersection of probability and dynamic systems. They offer a framework for modeling processes where the future state depends solely on the current state—a concept known as the Markov property."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://qian-jia.github.io/blogs/"},{"@type":"ListItem","position":2,"name":"An Introduction to Markov Chains","item":"https://qian-jia.github.io/blogs/intro-markov-chain/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"An Introduction to Markov Chains","name":"An Introduction to Markov Chains","description":"Markov chains represent a fascinating intersection of probability and dynamic systems. They offer a framework for modeling processes where the future state depends solely on the current state—a concept known as the Markov property.","keywords":["Markov Chain"],"articleBody":"Introduction and Context Markov chains represent a fascinating intersection of probability and dynamic systems. They offer a framework for modeling processes where the future state depends solely on the current state—a concept known as the Markov property. This memoryless nature simplifies complex systems, making them valuable across diverse domains from weather forecasting to financial modeling and text generation.\nHistorical Development The origins of Markov chains trace back to Andrei A. Markov, a Russian mathematician (1856-1922). His seminal work in 1906, “Rasprostranenie zakona bol’shih chisel na velichiny, zavisyaschie drug ot druga,” published in the Izvestiya Fiziko-matematicheskogo obschestva pri Kazanskom universitete, marked their formal introduction. This work extended probability theorems to sequences of dependent random variables—a significant advance beyond independent events. A 1971 reprint in R. Howard’s Dynamic Probabilistic Systems, volume 1: Markov Chains (Extension of the limit theorems of probability theory to a sum of variables connected in a chain) demonstrates its lasting importance. Markov first applied his theory to analyze letter sequences in Pushkin’s “Eugene Onegin,” as documented in a 2006 translation in Science in Context (An Example of Statistical Investigation of the Text Eugene Onegin Concerning the Connection of Samples in Chains), showcasing an early application to literary analysis.\nDefinition and Core Concepts Markov chains are Probabilistic Graphical Models (PGMs) that capture dynamic processes through state transitions over time. They are systems where the state at time $t+1$ depends only on the state at time $t$, independent of all prior states. Their formal definition includes:\nA set of states, $Q = {q_1, q_2, \\dots, q_n}$, representing possible conditions (e.g., Sunny, Cloudy, Rainy). A vector of prior probabilities, $\\Pi = {\\pi_1, \\pi_2, \\dots, \\pi_n}$, where $\\pi_i = P(S_0 = q_i)$, indicating initial state likelihoods. A transition matrix, $T = {p_{ij}}$, where $a_{ij} = P(S_t = q_j | S_{t-1} = q_i)$, capturing the probability of moving from state $i$ to state $j$. This structure ensures the Markov property, $P(S_t = q_j | S_{t-1} = q_i, S_{t-2} = q_k, \\dots) = P(S_t = q_j | S_{t-1} = q_i)$, emphasizing memorylessness. In the transition matrix, rows sum to 1 and entries range from 0 to 1. The state vector shows distribution at a point in time, with entries summing to 1. After $m$ transitions, the state vector evolves as $V_m = V_0T^m$, and some chains reach a steady-state, or equilibrium vector $E$, where $ET = E$.\nIllustrative Example Consider a weather model with states Sunny ($q_1$), Cloudy ($q_2$), and Rainy ($q_3$). The transition probabilities are:\nFrom Sunny: 70% chance of staying Sunny, 20% Cloudy, 10% Rainy. From Cloudy: 30% Sunny, 50% Cloudy, 20% Rainy. From Rainy: 20% Sunny, 30% Cloudy, 50% Rainy. This system is represented by a transition matrix $T$ and visualized through a state transition diagram, with nodes as states and arcs as transitions. Starting with an initial state vector—say, 100% Sunny—we can compute future distributions, demonstrating how the system evolves over discrete time steps while maintaining the Markov property.\ngraph LR; A[\"Sunny\"] --\u003e |0.2| B[\"Cloudy\"]; A --\u003e |0.7| A; A --\u003e |0.1| C[\"Rainy\"]; B --\u003e |0.3| A; B --\u003e |0.5| B; B --\u003e |0.2| C; C --\u003e |0.2| A; C --\u003e |0.3| B; C --\u003e |0.5| C; %% State transition probabilities for weather model %% Self-loops show probability of staying in same state %% Arrows between states show transition probabilities The diagram above illustrates the state transitions and their associated probabilities for our weather model. Each node represents a state (Sunny, Cloudy, or Rainy), and the directed edges show the probability of transitioning from one state to another. Self-loops indicate the probability of remaining in the same state.\n","wordCount":"598","inLanguage":"en","datePublished":"2025-04-03T00:00:00Z","dateModified":"2025-04-03T00:00:00Z","author":[{"@type":"Person","name":"Qian Jia"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://qian-jia.github.io/blogs/intro-markov-chain/"},"publisher":{"@type":"Organization","name":"Qian Jia","logo":{"@type":"ImageObject","url":"https://qian-jia.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{equation*}",right:"\\end{equation*}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1})})</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://qian-jia.github.io/ accesskey=h title="Qian Jia"><img src=https://qian-jia.github.io/favicon.ico alt aria-label=logo height=18 width=18>Qian Jia</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://qian-jia.github.io/papers/ title=Papers><span>Papers</span></a></li><li><a href=https://qian-jia.github.io/blogs/ title=Blog><span>Blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">An Introduction to Markov Chains</h1><div class=post-meta><span title='2025-04-03 00:00:00 +0000 UTC'>April 2025</span>&nbsp;&#183;&nbsp;Qian Jia</div></header><div class=post-content><h2 id=introduction-and-context>Introduction and Context</h2><p>Markov chains represent a fascinating intersection of probability and dynamic systems. They offer a framework for modeling processes where the future state depends solely on the current state—a concept known as the Markov property. This memoryless nature simplifies complex systems, making them valuable across diverse domains from weather forecasting to financial modeling and text generation.</p><h2 id=historical-development>Historical Development</h2><p>The origins of Markov chains trace back to Andrei A. Markov, a Russian mathematician (1856-1922). His seminal work in 1906, &ldquo;Rasprostranenie zakona bol&rsquo;shih chisel na velichiny, zavisyaschie drug ot druga,&rdquo; published in the <em>Izvestiya Fiziko-matematicheskogo obschestva pri Kazanskom universitete</em>, marked their formal introduction. This work extended probability theorems to sequences of dependent random variables—a significant advance beyond independent events. A 1971 reprint in R. Howard&rsquo;s <em>Dynamic Probabilistic Systems, volume 1: Markov Chains</em> (Extension of the limit theorems of probability theory to a sum of variables connected in a chain) demonstrates its lasting importance. Markov first applied his theory to analyze letter sequences in Pushkin&rsquo;s &ldquo;Eugene Onegin,&rdquo; as documented in a 2006 translation in <em>Science in Context</em> (<a href=https://doi.org/10.1017%2Fs0269889706001074 target=_blank>An Example of Statistical Investigation of the Text Eugene Onegin Concerning the Connection of Samples in Chains</a>), showcasing an early application to literary analysis.</p><h2 id=definition-and-core-concepts>Definition and Core Concepts</h2><p>Markov chains are Probabilistic Graphical Models (PGMs) that capture dynamic processes through state transitions over time. They are systems where the state at time $t+1$ depends only on the state at time $t$, independent of all prior states. Their formal definition includes:</p><ul><li>A set of states, $Q = {q_1, q_2, \dots, q_n}$, representing possible conditions (e.g., Sunny, Cloudy, Rainy).</li><li>A vector of prior probabilities, $\Pi = {\pi_1, \pi_2, \dots, \pi_n}$, where $\pi_i = P(S_0 = q_i)$, indicating initial state likelihoods.</li><li>A transition matrix, $T = {p_{ij}}$, where $a_{ij} = P(S_t = q_j | S_{t-1} = q_i)$, capturing the probability of moving from state $i$ to state $j$.</li></ul><p>This structure ensures the Markov property, $P(S_t = q_j | S_{t-1} = q_i, S_{t-2} = q_k, \dots) = P(S_t = q_j | S_{t-1} = q_i)$, emphasizing memorylessness. In the transition matrix, rows sum to 1 and entries range from 0 to 1. The state vector shows distribution at a point in time, with entries summing to 1. After $m$ transitions, the state vector evolves as $V_m = V_0T^m$, and some chains reach a steady-state, or equilibrium vector $E$, where $ET = E$.</p><h2 id=illustrative-example>Illustrative Example</h2><p>Consider a weather model with states Sunny ($q_1$), Cloudy ($q_2$), and Rainy ($q_3$). The transition probabilities are:</p><ul><li>From Sunny: 70% chance of staying Sunny, 20% Cloudy, 10% Rainy.</li><li>From Cloudy: 30% Sunny, 50% Cloudy, 20% Rainy.</li><li>From Rainy: 20% Sunny, 30% Cloudy, 50% Rainy.</li></ul><p>This system is represented by a transition matrix $T$ and visualized through a state transition diagram, with nodes as states and arcs as transitions. Starting with an initial state vector—say, 100% Sunny—we can compute future distributions, demonstrating how the system evolves over discrete time steps while maintaining the Markov property.</p><pre class=mermaid>
    graph LR;
    A[&#34;Sunny&#34;] --&gt; |0.2| B[&#34;Cloudy&#34;];
    A --&gt; |0.7| A;
    A --&gt; |0.1| C[&#34;Rainy&#34;];
    B --&gt; |0.3| A;
    B --&gt; |0.5| B;
    B --&gt; |0.2| C;
    C --&gt; |0.2| A;
    C --&gt; |0.3| B;
    C --&gt; |0.5| C;

%% State transition probabilities for weather model
%% Self-loops show probability of staying in same state
%% Arrows between states show transition probabilities
</pre><p>The diagram above illustrates the state transitions and their associated probabilities for our weather model. Each node represents a state (Sunny, Cloudy, or Rainy), and the directed edges show the probability of transitioning from one state to another. Self-loops indicate the probability of remaining in the same state.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://qian-jia.github.io/tags/markov-chain/>Markov Chain</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://qian-jia.github.io/>Qian Jia</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script type=module>
      import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
      mermaid.initialize({ startOnLoad: true });
    </script></body></html>