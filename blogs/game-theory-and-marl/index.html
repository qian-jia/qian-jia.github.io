<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>博弈论与多智能体强化学习（MARL）术语对比 | Qian Jia</title>
<meta name=keywords content="Game Theory,MARL,Terminology"><meta name=description content="本文整理了一份详细的术语对比表，帮助理解博弈论和多智能体强化学习在概念上的联系与区别。"><meta name=author content="Qian Jia"><link rel=canonical href=https://qian-jia.github.io/blogs/game-theory-and-marl/><link rel=stylesheet href=https://chinese-fonts-cdn.deno.dev/packages/LxgwNeoZhiSong/dist/LXGWNeoZhiSong/result.css><link crossorigin=anonymous href=/assets/css/stylesheet.d6c5f5ed0d983959e3280265badf3b545ec092c32db95aaa0b66c656531367e5.css integrity="sha256-1sX17Q2YOVnjKAJlut87VF7AksMtuVqqC2bGVlMTZ+U=" rel="preload stylesheet" as=style><link rel=icon href=https://qian-jia.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://qian-jia.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://qian-jia.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://qian-jia.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://qian-jia.github.io/blogs/game-theory-and-marl/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="博弈论与多智能体强化学习（MARL）术语对比"><meta property="og:description" content="本文整理了一份详细的术语对比表，帮助理解博弈论和多智能体强化学习在概念上的联系与区别。"><meta property="og:type" content="article"><meta property="og:url" content="https://qian-jia.github.io/blogs/game-theory-and-marl/"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2025-03-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-30T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="博弈论与多智能体强化学习（MARL）术语对比"><meta name=twitter:description content="本文整理了一份详细的术语对比表，帮助理解博弈论和多智能体强化学习在概念上的联系与区别。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://qian-jia.github.io/blogs/"},{"@type":"ListItem","position":2,"name":"博弈论与多智能体强化学习（MARL）术语对比","item":"https://qian-jia.github.io/blogs/game-theory-and-marl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"博弈论与多智能体强化学习（MARL）术语对比","name":"博弈论与多智能体强化学习（MARL）术语对比","description":"本文整理了一份详细的术语对比表，帮助理解博弈论和多智能体强化学习在概念上的联系与区别。","keywords":["Game Theory","MARL","Terminology"],"articleBody":"博弈论（Game Theory）和多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）是两个密切相关的领域。博弈论研究多个理性决策者之间的交互，而 MARL 研究多个智能体如何通过学习优化其策略。在许多应用中，例如机器人协作、自动驾驶、网络安全等，这两者都发挥着关键作用。\n本文整理了一份详细的术语对比表，帮助理解二者在概念上的联系与区别。\n术语对比表 博弈论（Game Theory） 多智能体强化学习（MARL） 解释 玩家（Player） 智能体（Agent） 参与决策的个体。博弈论中是理性决策者，MARL 中是通过学习不断改进行为的智能体。 策略（Strategy） 策略（Policy） 决策规则，博弈论中是预定义的，MARL 中通常是学习得到的。 收益（Payoff） 回报（Reward） 采取某个行动后的收益，MARL 中可以是即时奖励或长期回报。 策略组合（Strategy Profile） / Profile 联合策略（Joint Policy） 所有玩家/智能体的策略组合。策略 Profile 影响整体博弈或学习过程。 最优策略（Optimal Strategy） 最优策略（Optimal Policy） 使得收益最大化的策略，在 MARL 中通常通过学习逼近。 纳什均衡（Nash Equilibrium） 均衡策略（Equilibrium Policy） 各方无利可图改变自身策略的稳定状态。 支配策略（Dominant Strategy） 最佳响应策略（Best Response Policy） 在任何情况下总能获得较好收益的策略；MARL 中常用于竞争环境中的最优学习。 子博弈完美均衡（Subgame Perfect Equilibrium） 逐步最优策略（Sequential Optimal Policy） 动态博弈中，每个阶段都达到最优的均衡解。MARL 需保证每个时间步的策略一致性。 合作博弈（Cooperative Game） 合作 MARL（Cooperative MARL） 玩家/智能体可以合作以实现共同目标。 非合作博弈（Non-Cooperative Game） 竞争 MARL（Competitive MARL） 玩家/智能体之间存在竞争关系，优化自身利益。 混合策略（Mixed Strategy） 随机策略（Stochastic Policy） 以概率分布选择行动。 纯策略（Pure Strategy） 确定性策略（Deterministic Policy） 在每种情形下总是采取相同的行动。 极小极大（Minimax） Minimax Q-learning 在零和博弈中最小化损失，MARL 中用于对抗性学习。 博弈矩阵（Payoff Matrix） 状态-动作价值表（Q-Table） 描述收益的表格，在 Q-learning 中类似于 Q-table。 重复博弈（Repeated Game） 时间步序列（Episodic Learning） 参与者进行多次交互，MARL 通过多个时间步或回合进行学习。 马尔可夫博弈（Markov Game / Stochastic Game） 多智能体马尔可夫决策过程（Multi-Agent MDP） 在多智能体决策中引入马尔可夫过程，为 MARL 提供理论框架。 演化稳定策略（Evolutionarily Stable Strategy, ESS） 经验回放（Experience Replay） 通过历史经验优化策略，在 MARL 中用于训练智能体。 信息完备性（Perfect Information） 完全观测环境（Fully Observable Environment） 所有信息均已知。 信息不完全（Imperfect Information） 部分观测环境（Partially Observable Environment, POMDP） 只能观察部分状态信息。 信号传递（Signaling） 通信协议（Communication Protocol） 在信息不对称场景下，博弈论关注信号如何传递；在 MARL 中，智能体可以通过通信协作。 均衡选择（Equilibrium Selection） 策略协调 / 协同机制（Policy Coordination / Coordination Mechanism） 存在多个均衡解时，如何达成共识。MARL 中可以通过奖励设计和协议制定来引导协调。 合作机制（Cooperation Mechanism） 联合奖励设计（Joint Reward Design） 针对合作问题，通过设计共享奖励促进整体最优；在 MARL 中通过联合奖励和信用分配实现团队协作。 信念更新（Belief Updating） 对手建模 / 他人策略推断（Opponent Modeling / Policy Inference） 通过贝叶斯更新等方法修正对其他参与者的信念；MARL 中智能体可能学习估计其他智能体的行为，以更好地协同或竞争。 ","wordCount":"169","inLanguage":"en","datePublished":"2025-03-30T00:00:00Z","dateModified":"2025-03-30T00:00:00Z","author":[{"@type":"Person","name":"Qian Jia"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://qian-jia.github.io/blogs/game-theory-and-marl/"},"publisher":{"@type":"Organization","name":"Qian Jia","logo":{"@type":"ImageObject","url":"https://qian-jia.github.io/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{equation*}",right:"\\end{equation*}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1})})</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://qian-jia.github.io/ accesskey=h title="Qian Jia"><img src=https://qian-jia.github.io/favicon.ico alt aria-label=logo height=18 width=18>Qian Jia</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://qian-jia.github.io/papers/ title=Papers><span>Papers</span></a></li><li><a href=https://qian-jia.github.io/blogs/ title=Blog><span>Blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">博弈论与多智能体强化学习（MARL）术语对比</h1><div class=post-meta><span title='2025-03-30 00:00:00 +0000 UTC'>March 2025</span>&nbsp;&#183;&nbsp;Qian Jia</div></header><div class=post-content><p>博弈论（Game Theory）和多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）是两个密切相关的领域。博弈论研究多个理性决策者之间的交互，而 MARL 研究多个智能体如何通过学习优化其策略。在许多应用中，例如机器人协作、自动驾驶、网络安全等，这两者都发挥着关键作用。</p><p>本文整理了一份详细的术语对比表，帮助理解二者在概念上的联系与区别。</p><h2 id=术语对比表>术语对比表</h2><table><thead><tr><th><strong>博弈论（Game Theory）</strong></th><th><strong>多智能体强化学习（MARL）</strong></th><th><strong>解释</strong></th></tr></thead><tbody><tr><td><strong>玩家（Player）</strong></td><td><strong>智能体（Agent）</strong></td><td>参与决策的个体。博弈论中是理性决策者，MARL 中是通过学习不断改进行为的智能体。</td></tr><tr><td><strong>策略（Strategy）</strong></td><td><strong>策略（Policy）</strong></td><td>决策规则，博弈论中是预定义的，MARL 中通常是学习得到的。</td></tr><tr><td><strong>收益（Payoff）</strong></td><td><strong>回报（Reward）</strong></td><td>采取某个行动后的收益，MARL 中可以是即时奖励或长期回报。</td></tr><tr><td><strong>策略组合（Strategy Profile） / Profile</strong></td><td><strong>联合策略（Joint Policy）</strong></td><td>所有玩家/智能体的策略组合。策略 Profile 影响整体博弈或学习过程。</td></tr><tr><td><strong>最优策略（Optimal Strategy）</strong></td><td><strong>最优策略（Optimal Policy）</strong></td><td>使得收益最大化的策略，在 MARL 中通常通过学习逼近。</td></tr><tr><td><strong>纳什均衡（Nash Equilibrium）</strong></td><td><strong>均衡策略（Equilibrium Policy）</strong></td><td>各方无利可图改变自身策略的稳定状态。</td></tr><tr><td><strong>支配策略（Dominant Strategy）</strong></td><td><strong>最佳响应策略（Best Response Policy）</strong></td><td>在任何情况下总能获得较好收益的策略；MARL 中常用于竞争环境中的最优学习。</td></tr><tr><td><strong>子博弈完美均衡（Subgame Perfect Equilibrium）</strong></td><td><strong>逐步最优策略（Sequential Optimal Policy）</strong></td><td>动态博弈中，每个阶段都达到最优的均衡解。MARL 需保证每个时间步的策略一致性。</td></tr><tr><td><strong>合作博弈（Cooperative Game）</strong></td><td><strong>合作 MARL（Cooperative MARL）</strong></td><td>玩家/智能体可以合作以实现共同目标。</td></tr><tr><td><strong>非合作博弈（Non-Cooperative Game）</strong></td><td><strong>竞争 MARL（Competitive MARL）</strong></td><td>玩家/智能体之间存在竞争关系，优化自身利益。</td></tr><tr><td><strong>混合策略（Mixed Strategy）</strong></td><td><strong>随机策略（Stochastic Policy）</strong></td><td>以概率分布选择行动。</td></tr><tr><td><strong>纯策略（Pure Strategy）</strong></td><td><strong>确定性策略（Deterministic Policy）</strong></td><td>在每种情形下总是采取相同的行动。</td></tr><tr><td><strong>极小极大（Minimax）</strong></td><td><strong>Minimax Q-learning</strong></td><td>在零和博弈中最小化损失，MARL 中用于对抗性学习。</td></tr><tr><td><strong>博弈矩阵（Payoff Matrix）</strong></td><td><strong>状态-动作价值表（Q-Table）</strong></td><td>描述收益的表格，在 Q-learning 中类似于 Q-table。</td></tr><tr><td><strong>重复博弈（Repeated Game）</strong></td><td><strong>时间步序列（Episodic Learning）</strong></td><td>参与者进行多次交互，MARL 通过多个时间步或回合进行学习。</td></tr><tr><td><strong>马尔可夫博弈（Markov Game / Stochastic Game）</strong></td><td><strong>多智能体马尔可夫决策过程（Multi-Agent MDP）</strong></td><td>在多智能体决策中引入马尔可夫过程，为 MARL 提供理论框架。</td></tr><tr><td><strong>演化稳定策略（Evolutionarily Stable Strategy, ESS）</strong></td><td><strong>经验回放（Experience Replay）</strong></td><td>通过历史经验优化策略，在 MARL 中用于训练智能体。</td></tr><tr><td><strong>信息完备性（Perfect Information）</strong></td><td><strong>完全观测环境（Fully Observable Environment）</strong></td><td>所有信息均已知。</td></tr><tr><td><strong>信息不完全（Imperfect Information）</strong></td><td><strong>部分观测环境（Partially Observable Environment, POMDP）</strong></td><td>只能观察部分状态信息。</td></tr><tr><td><strong>信号传递（Signaling）</strong></td><td><strong>通信协议（Communication Protocol）</strong></td><td>在信息不对称场景下，博弈论关注信号如何传递；在 MARL 中，智能体可以通过通信协作。</td></tr><tr><td><strong>均衡选择（Equilibrium Selection）</strong></td><td><strong>策略协调 / 协同机制（Policy Coordination / Coordination Mechanism）</strong></td><td>存在多个均衡解时，如何达成共识。MARL 中可以通过奖励设计和协议制定来引导协调。</td></tr><tr><td><strong>合作机制（Cooperation Mechanism）</strong></td><td><strong>联合奖励设计（Joint Reward Design）</strong></td><td>针对合作问题，通过设计共享奖励促进整体最优；在 MARL 中通过联合奖励和信用分配实现团队协作。</td></tr><tr><td><strong>信念更新（Belief Updating）</strong></td><td><strong>对手建模 / 他人策略推断（Opponent Modeling / Policy Inference）</strong></td><td>通过贝叶斯更新等方法修正对其他参与者的信念；MARL 中智能体可能学习估计其他智能体的行为，以更好地协同或竞争。</td></tr></tbody></table></div><footer class=post-footer><ul class=post-tags><li><a href=https://qian-jia.github.io/tags/game-theory/>Game Theory</a></li><li><a href=https://qian-jia.github.io/tags/marl/>MARL</a></li><li><a href=https://qian-jia.github.io/tags/terminology/>Terminology</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://qian-jia.github.io/>Qian Jia</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>